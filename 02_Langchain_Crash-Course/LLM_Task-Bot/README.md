# Getting response from different LLMs by creating API's with helps of langchain , langserve , FastAPI
This project provides ... (brief description of what the project does).

## Requirements

This project requires the following dependencies:

1. **Ollama:** To start the backend server. Installation instructions: (Provide instructions on how to install Ollama, including links if necessary)
2. **Python 3.x:** The scripting language used for the project. Installation instructions: https://www.python.org/downloads/
3. **Streamlit:** To run the user interface. Installation instructions: `pip install streamlit`

**Important Note:** It is essential to install and configure all three dependencies before running the project.

## Running the Project

1. **Start the Ollama Server:**
   Open a terminal and run the command:

   ```bash
   ollama start

2. **Start the  langserve:**
   Open a terminal and run the command:

   ```bash
   python app.py

3. **Start the main file and UI :**
   Open a terminal and run the command:

   ```bash
   streamlit run client.py

